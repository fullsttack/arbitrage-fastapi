version: '3.8'

services:
  db:
    image: postgres:16
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=crypto_arbitrage
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  web:
    build: .
    command: .venv/bin/gunicorn config.wsgi:application --bind 0.0.0.0:8000 --workers 4 --timeout 120
    volumes:
      - .:/app
      - static_volume:/app/staticfiles
      - media_volume:/app/media
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # High-priority arbitrage workers
  celery-arbitrage:
    build: .
    command: .venv/bin/celery -A config worker -Q arbitrage_scan,arbitrage_execution,execution_monitoring -l info --concurrency=2 -n arbitrage@%h
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", ".venv/bin/celery", "-A", "config", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # Market data workers
  celery-market:
    build: .
    command: .venv/bin/celery -A config worker -Q market_data,validation -l info --concurrency=3 -n market@%h
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", ".venv/bin/celery", "-A", "config", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # Trading workers
  celery-trading:
    build: .
    command: .venv/bin/celery -A config worker -Q trading -l info --concurrency=2 -n trading@%h
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", ".venv/bin/celery", "-A", "config", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # Analytics and reporting workers
  celery-analytics:
    build: .
    command: .venv/bin/celery -A config worker -Q analytics,reports,risk_analysis -l info --concurrency=2 -n analytics@%h
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", ".venv/bin/celery", "-A", "config", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # Background maintenance workers
  celery-background:
    build: .
    command: .venv/bin/celery -A config worker -Q cleanup,monitoring,optimization,backup -l info --concurrency=1 -n background@%h
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", ".venv/bin/celery", "-A", "config", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # Celery beat scheduler
  celery-beat:
    build: .
    command: .venv/bin/celery -A config beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  # Flower monitoring
  flower:
    build: .
    command: .venv/bin/celery -A config flower --port=5555 --broker=redis://redis:6379/1
    volumes:
      - .:/app
    ports:
      - "5555:5555"
    env_file:
      - .env
    environment:
      - FLOWER_BASIC_AUTH=admin:arbitrage2024
    depends_on:
      - celery-arbitrage
    restart: unless-stopped

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sites-available:/etc/nginx/sites-available:ro
      - static_volume:/var/www/static
      - media_volume:/var/www/media
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - web
    restart: unless-stopped

  # Redis Commander for debugging
  redis-commander:
    image: rediscommander/redis-commander:latest
    hostname: redis-commander
    ports:
      - "8081:8081"
    environment:
      - REDIS_HOSTS=local:redis:6379
    depends_on:
      - redis
    profiles:
      - debug

  # PostgreSQL admin
  pgadmin:
    image: dpage/pgadmin4
    ports:
      - "8080:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@arbitrage.local
      - PGADMIN_DEFAULT_PASSWORD=admin123
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      - db
    profiles:
      - debug

  # Prometheus monitoring
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring

  # Grafana dashboards
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    profiles:
      - monitoring

  # Log aggregation
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    profiles:
      - logging

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    profiles:
      - logging

  # System performance monitoring
  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    profiles:
      - monitoring

volumes:
  postgres_data:
  redis_data:
  static_volume:
  media_volume:
  pgadmin_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:

networks:
  default:
    driver: bridge